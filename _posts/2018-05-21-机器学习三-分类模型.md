#### 逻辑回归模型

前面介绍的线性回归模型适用于输出为连续值的情景，比如输出房价。有的场景中输出会是离散值，比如图像识别。这时候我们就可以用分类模型来做，如逻辑回归模型可以用于处理二分类任务，而softmax回归模型应用更为广泛，用来处理多分类任务。

逻辑回归模型实际上是将线性回归模型的输出值通过sigmoid函数（S函数）映射到[0,1]之间。这样就可以用来表示一个概率，通常输出大于0.5我们就认为y=1,反之则为0。（当然阈值不一定是0.5）。
$$
z=wx+b,\\
g(z)=\frac{1}{1+e^{z}}
$$
![posts_neuron_sigmoid](/img/posts_ML/posts_neuron_sigmoid.png)

逻辑回归的损失函数使用了交叉熵损失函数，定义如下：
$$
Cost(h_{\theta}(x),y) = \begin{equation}
\left\{\begin{array}{lr}
-log(h_{\theta}(x)),\quad if\quad y=1 \\
-log(1-h_{\theta}(x)),\quad if \quad y = 0\\      
 \end{array}  
\right.  
\end{equation}
$$
简化为：
$$
\begin{array}{lr}
J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_{\theta}(x_i),y_i) \\
\quad\quad=-\frac{1}{m}\sum_{i=1}^m(y_ilogh_{\theta}(x_i) + (1-y_{i})log(1-h_{\theta}(x_i)))
 \end{array}
$$
用梯度下降算法训练时沿着最小化$J(\theta)$更新参数:
$$
\begin{array}{lr}\theta_j = \theta_j-\alpha\frac{\partial J(\theta)}{\partial \theta_j}\\
=\theta_j - \alpha \sum_{i=1}^m(h_\theta(x_{i})-y_i)x_i
\end{array}
$$


#### softmax回归模型

softmax回归用于处理多分类问题，实际上比逻辑回归应用广泛的很多，也是一个单层的神经网络。和线性回归不同的是的softmax回归输出层中的输出个数等于类别个数，从一个变成了多个。

例如在一个2*2图片样本分类任务中，设$w_{(i)}, b_{(i)}$分别为softmax回归的权重和偏差参数。给定单个图片的输入特征$x_1, x_2, x_3, x_4$,有 
$$
o_1=x_1w_{11}+x_2w_{21}+x_3w_{31}+x_4w_{41}+b_1,\\
o_2=x_1w_{12}+x_2w_{22}+x_3w_{32}+x_4w_{42}+b_2,\\
o_3=x_1w_{13}+x_2w_{23}+x_3w_{33}+x_4w_{43}+b_3,
$$
在softmax回归中$y_1,y_2,y_3$的计算都要依赖于$x_1,x_2,x_3,x_4$。所以softmax的输出层是一个全连接层。

![posts_three_softmax](/img/posts_ML/posts_three_softmax.png)

在得到输出层的三个输出后，要预测输出分别为猫，狗，兔子的概率。设它们分别为$y^{'}_1,y^{'}_2,y^{'}_3​$，对$o_1,o_2,o_3​$做softmax运算得到模型的最终输出：
$$
y^{'}_1=\frac{e^{o_1}}{\sum_{i=1}^{3}e^{o_i}},\\
y^{'}_2=\frac{e^{o_2}}{\sum_{i=1}^{3}e^{o_i}},\\
y^{'}_3=\frac{e^{o_3}}{\sum_{i=3}^{3}e^{o_i}}
$$
由于$y^{'}_1+y^{'}_2+y^{'}_3=1， y^{'}_1>=0,y^{'}_2>=0y^{'}_3>=0$,故$y^{'}_1,y^{'}_2,y^{'}_3$是一个合法的概率分布。可将上面softmax运算中的三个等式记为
$$
y^{'}_1,y^{'}_2,y^{'}_3=Softmax(o^{'}_1,o^{'}_2,o^{'}_3)
$$
相对应的矢量计算表达式为：
$$
\vec o^{(i)}=\vec x^{(i)}\vec W + \vec b,\\
y^{'o_{(i)}}=Softmax(\vec o^{(i)})
$$

#### 交叉熵损失函数

softmax回归模型使用交叉熵损失函数，直观上，训练集上每个样本的真实标签的被预测概率越大，分类越准确。设样本$i$的标签的被预测概率为$P_i$，例如，如果样本$i$的标签为$y_3$，那么$P_i=y^{'}_3$。假设训练集样本数为N，我们希望最小化
$$
l(\Theta)=-\frac{1}{N}\sum_{i=1}^NlogP_i
$$
训练好softmax回归模型后，给定任一样本特征，我们可以预测每个输出类别的概率。通常把预测概率最大的类别作为输出类别，使用准确率（正确预测数量与总预测数量的比值）来评估模型的好坏。