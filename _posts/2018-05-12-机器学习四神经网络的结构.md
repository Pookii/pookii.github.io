#### 神经网络的结构

前面介绍的线性回归和softmax回归都是单层神经网络。多层神经网络在单层的基础上引入了一个到多个隐藏层。隐藏层位于输入层和输出层之间。多层感知机是最基础的深度学习模型。

![posts_four_arch](/Users/hj/Desktop/pookii.github.io/img/posts_ML/posts_four_arch.png)

在这里我们所讲的神经网络是前馈的也就是说没有环。在一些其他的神经网络中比如RNN就是有环的，它在隐藏层加了一个反馈连接，也就是说隐含层当前时刻的输入有一部分是前一时刻的隐含层输出。这就使得RNN可以通过循环反馈连接保留前面所有时刻的信息，拥有了记忆功能。所以RNN非常适合用于对时序信号的建模。

![posts_four_rnn](/Users/hj/Desktop/pookii.github.io/img/posts_ML/posts_four_rnn.png)

#### 仿射变换

多层感知机输出层和单层神经网络的输出层的计算类似，只是输出层的输入是隐藏层的输出。我们通常将隐藏层的输出称为隐藏层变量或隐藏变量。

给定一个批量大小为n的样本，输入个数为$x$，输出个数为$y$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$，隐藏变量为$\vec H(nxh)$。假设输出层的权重和偏差参数分别为$\vec W_o(h×y), \vec b_o(1×y)$，多层感知机输出
$$
\vec O=\vec H\vec W_o+\vec b_o
$$
实际上，多层感知机的输出$\vec O$是对上一层的输出$\vec H$的仿射变换。它包括一次通过乘以权重参数的线性变换和一次通过加上偏差参数的平移。

然而如果隐藏层也只对输入做仿射变换，可以假设单个样本特征为$x$(1x$x$)，隐藏层的权重参数和偏差参数分别为

现在假设我们要解决一个针对手写体数字的图像分类问题。我们可以设计出如下的神经网络，输入是一些28*28的二值图像，因此输入层有784个神经元，每个神经元对应二值图像中的一个像素点的灰度值。隐含层的神经元个数可以根据神经网络的表现调整。输出层是10个神经元用来表述是否为某个数字，比如第一个神经元输出是0.99那我们就认为神经网络预测结果是0。

![posts_three_mnist](/Users/hj/Desktop/pookii.github.io/img/posts_ML/posts_three_mnist.png)

在训练神经网络解决问题时，我们需要大量带有标注的数据作为训练集，然后用没有出现在训练集中的数据作为测试集。用神经网络在测试集上的表现评估好坏。这里我们使用均方误差（线性回归问题有提到）来作为损失函数，解决这个问题的过程就是使均方函数最小的过程。可是我们不是在解决一个图像分类任务吗，为什么要转换成求解损失函数最小值这个问题呢？

实际上就这个图像分类任务而言，分类正确的图像个数和w,b之间并没有一种平滑的函数关系能描述。微调w,b的值对正确分类的图像数并没有什么影响。这时候我们用和w,b之前有平滑函数关系的函数比如常见的均方误差函数来代替，反而更容易构建和理解。这个函数我们之前已经详细讲过，下面主要来介绍一下训练过程中我们是如何求解最小值的。

抛去函数的具体形式不说，由微积分的知识我们知道，函数沿梯度方向增长的最快。



