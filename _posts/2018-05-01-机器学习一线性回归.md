机器学习为我们提供了一种新的解决问题的思维方式——让机器像人一样通过学习以往经验来提升处理某一系列问题的性能。一般来说，机器学习分为两类：

**监督学习**

监督学习指机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测。这一过程和小孩子的学习过程类似：如果做对了，我们会表扬；反之则批评。因而，我们利用带有正确标签的数据集来训练机器学习。

监督学习问题又分为两种：

- 回归问题：解决的问题具有连续性，如学习房子面积和房价的关系来预测房价。
- 分类问题：解决离散分类问题，如图片里是猫还是狗 。

**无监督学习**

我们对不为训练集提供对应的类别标识，机器通过学习归纳自动将数据按照某种（或多种）属性分为不同的集合。如将数万条新闻自动按照中心词，字数等特征分组。

------

**线性回归问题**：

线性回归问题是指用一条直线来拟合数据分布，实际是对特征方程中参数的求解。我们先来看一个简单的例子：

![post_linear_regression_01](/img/post_linear_regression_01.png)

假设房子价格只与其面积间存在线性关系，特征方程为：


$$
y = wx + b
$$

我们拥有一些历史数据 $(x_1, y_1), (x_2, y_2), …(x_N, y_N)$一开始先随机猜测一组$w$和$b$的值$w_0$, $b_0$，则将$x_0$代入方程求得预测值



$$
y' = w_0  x_0 + b
$$



通常将预测值和真实值间的差称作损失。为了评估预测的效果，我们会在机器学习的过程中定义损失函数来评估预测的效果。总之，我们希望机器通过学习使得损失不断减小。而数据规模比较大时，为减少计算过程，尽量使损失减小的越快越好。由微积分的知识我们知道，沿着梯度方向函数值增大的速度最快，因而只要沿损失函数梯度下降的方向更新参数即可。当损失几乎不再变化时，即损失函数收敛到某个值，学习过程便终止。回归问题常用的损失函数是$L2$（均方误差函数）：

$$
L = \frac{\sum_{i=1}^N(y'-y)^2}{N}
$$


我们试图让均方误差最小化，即


$$
(w', b') = arg  min\sum_{i=1}^N(f(x_i) - y_i)^2
$$

$$
							= arg  min\sum_{i=1}^N(y_i - wx_i - b)^2
$$

均方误差对应了常用的欧式距离，基于均方误差最小化来进行模型求解的方法称为“最小二乘法”，线性回归问题中，最小二乘法是试图找到一条直线，使所有样本到直线的欧式距离之和最小。

求解$w, b$使得$L_{(w,b)}= \sum_{i=1}^N(y_i - wx_i - b)^2$最小化的过程，称为线性回归模型的最小二乘参数估计。将$L_(w, b)$分别对$w, b$求导，得


$$
\frac{\partial L_{(w,b)}}{\partial w} = 2(w\sum_{i=1}^Nx_i^2 - \sum_{i=1}^N(y_i-b)x_i ) = 0
$$

$$
\frac{\partial L_{(w,b)}}{\partial b} = 2(Nb - \sum_{i=1}^N(y_i-wx_i ) = 0
$$

解得


$$
w = \frac{\sum_{i=i}^Ny_i(x_i-\bar x)}{\sum_{i=1}^Nx_i^2 - \frac{1}{N}(\sum_{i=1}^mx_i)^2}
$$

$$
b = \frac{1}{N}\sum_{i=1}^N(y_i-wx_i)
$$

接下来我们考虑更一般的情形：对房价的影响除了房间面积之外还有地理位置，交通，教育资源等等。假设样本共有$d$个这样的属性描述,用一个d维列向量$\vec{x}$表示，令


$$
X = (\vec{x_1}, \vec{x_2}, ... \vec{x_n})
$$

$$
\vec{y} = (y_1, y_2, ... x_n)^T
$$

$$
\vec{w} = (w_1, w_2, ... w_d)^T
$$

试图学得


$$
F(X) = \vec{w} X + b
$$



令


$$
\vec{w_1} = (w_1, w_2, … w_d, b)^T
$$

$$
X_1 =  \begin{bmatrix}x_1&x_2&...&x_n \\1&1&…&1\end{bmatrix}
$$



则 


$$
F(X_1) = \vec{w_1} X_1
$$

$$
L(\vec{w_1}) = \frac{1}{n} (\vec{w_1}X_1- \vec{y})(\vec{w_1}X_1- \vec{y})^T
$$



对$\vec{w_1}$求导得


$$
\frac{\partial L}{\partial \vec{w_1}} = \frac{2}{n}(\vec{w_1}X_1 - \vec{y})X_1^T = 0
$$


解得


$$
\vec{w_1} = \vec{y}X_1^T (X_1X_1^T)^{-1}
$$


这样我们就得到了$\vec{w}$的解析解。不过现实任务中$X_1X_1^T$通常不是满秩矩阵，如在很多任务中有大量变量甚至超过了样例数，导致$X$的行数多于列数，此时就可以解出多个 $\vec{w}$ ，它们都能使均方误差最小化，选择哪一个解作为输出将由学习算法的归纳偏好决定，常见的做法是引入正则化项。
