perceptron 和 sigmoid neuro是两种非常重要的人造神经元

#### perceptron

perceptron被发明于1950-1960年，现在比较常用的其实是sigmoid neuro，不过要想更好的理解sigmoid neuro为什么那么定义，还是要先明白什么是perceptron。

一个感知器通常有若干个二进制输入$x_1, x_2, …$ 只有一个二进制值作为输出。每个输入都有一个对应的实数值$w_i$来表示输入的权重。输出定义为：
$$
output = \begin{equation}
\left\{\begin{array}{lr}
0,\quad if \sum_j{w_jx_j} \le threshold \\
1,\quad if\sum_j{w_jx_j} >threshold \\      
 \end{array}  
\right.  
\end{equation}
$$
假如在周末会有一场陈奕迅的演唱会，你是否会去受以下三个因素影响：

1.门票价格是否符合预期

2.女/男朋友是否同行

3.交通是否便利

在这些因素中你最在意的是演出门票的价格，那么就可以用感知器来帮你做决定。记门票符合预期，女/男朋友同行，交通便利这些事件为1，反之为0。我们可以让门票价格这个因素的权重为6，女/男朋友同行的权重为2，交通便利的权重为2。令threshold为5，则只要门票价格符合预期，你就一定会去。反之则不会去。如果让threshold为3，则你会在门票价格符合预期或者女/男朋友同行和交通便利同时满足的时候去。threshold降低表明你想要去看演唱会的意愿更强烈。

尽管感知器无法真正像人类那样考虑错综复杂的因素做出决定，不过我们还是通过巧妙设计让它发挥更大的作用。如下图所示，感知器的第一层可以根据输入因素做一些简单粗暴的输出决定，第二层对第一层输出的结果加上不同的权重进行考虑作出相对复杂的决定，还可以加入更多的层来模拟更为复杂的设定。

![posts_neuron_p](/img/posts_ML/posts_neuron_p.png)

我们可以进一步简化感知器的表示：$\sum_j{w_jx_j}$是向量的点乘可以用$\vec w \cdot \vec x$表示，然后把threshold移到不等式左边，通常称之为感知器的偏差$ bias \equiv -threshold$，于是感知器的表达式就转换成：
$$
output = \begin{equation}
\left\{\begin{array}{lr}
0,\quad if \vec w \cdot \vec x + b \le 0 \\
1,\quad if\vec w \cdot \vec x + b> 0 \\      
 \end{array}  
\right.  
\end{equation}
$$
bias可以理解为使输出为1的容易程度，或者生物角度上使感知器激活的容易程度。

感知器不仅可以根据输入因素做决定，而且可以用来做一些基本的逻辑运算，如：与，或，与非等

例如我们有一个带有两个输入项的感知器，偏差为3，输入项权重都是-2，则输入（1，1）得到输出0，而输入（0，1），（1，0）或 （0，0）得到输出1，这其实就是与非门。实际上我们可以用感知器来进行任何逻辑运算。比如，可以用与非门搭建一个进行位加的逻辑电路，

![posts_neuron_add](/img/posts_ML/posts_neuron_add.png)

将上图的与非门换成感知器表示：

![posts_neuron_add_1](/img/posts_ML/posts_neuron_add_1.png)

将相同的输入合并，同时将输入作为新的一层——输入层加入感知器，得到：

其中未标明的输入权重都是 -2，只有右下角是-4，偏差都是3

![posts_neuron_bit_add](/img/posts_ML/posts_neuron_bit_add.png)

感知器的输入层因为没有输入，则$\vec w \cdot \vec x$ = 0,结果恒为b。可以看作一种能输出固定值$x_1, x_2, …$的感知器。

虽然与非门在逻辑计算中十分常用，但是感知器如果仅仅只能模拟逻辑电路，那也只不过是另一种与非门罢了。幸运的是，通过编写各种学习算法，我们可以让感知器在无人干预的情况下自动调整权重和偏差这些参数，从而解决更多无法直接设计逻辑电路来解决的问题。

#### sigmord neurons

设计学习算法，我们希望通过微调权重和偏差值来使输出结果作出微小改变。比如，我们用感知器组成的神经网络来识别图像中的动物。当感知器误将狗识别成猫的时候，我们修改权重和偏差值使得结果更倾向于输出猫这个类别。不断微调重复这个过程，使得结果越来约符合期望。但是如果使用感知器，微小的改动权重或者偏差很可能使得输出结果完全相反。从而对后面层的结果产生复杂不可控的影响。为了解决这个问题，介绍一种新的人工神经元：sigmord neurons

sigmord neurons和感知器一样具有输入$x_1, x_2, …$以及相对应的权重值，偏差。不同的是，输入项可以是介于0和1的任意实数。输出项也不再是0或者1而是$\sigma(\vec w \cdot \vec x + b)$ ，这个函数被称为sigmord函数。
$$
\sigma(z) \equiv \frac{1}{1+e^{-z}}
$$
也就是说当我们有输入$x_1, x_2, …$权重值$w_1, w_2, …$，偏差$b$时，神经元输出的结果为：
$$
\frac{1}{1+e^{-\sum_jw_jx_j - b}}
$$
假设$z\equiv \vec w \cdot \vec x + b$为一个很大的正数，那么输出将接近为0，反之如果$z\equiv \vec w \cdot \vec x + b$是一个很小的负数则结果将接近等于1。实际上$\sigma$ 具体的代数表达式是什么并不重要，关键是它的函数形状：

![posts_neuron_sigmoid](/img/posts_ML/posts_neuron_sigmoid.png)

可以看到sigmord函数是平缓的阶梯函数，而perceptron的函数图形和阶梯函数只有一点之差（z = 0处阶梯函数值是1）。

![posts_ neuron_step_function](/img/posts_ML/posts_ neuron_step_function.png)

这样一来，对于$w, b$的微小变化输出也只是微小改变。实际上由微积分的知识我们可以计算出
$$
\Delta output \approx \sum_j\frac{\partial output}{\partial w_j}\Delta w_j + \frac{\partial output}{\partial b}\Delta b
$$
前文我们也说过$\sigma$函数的具体函数式并不重要，那为什么要用$\frac{1}{1+e^{-\sum_jw_jx_j - b}}$呢？实际上仅仅是因为指数函数求导运算比较方便！除了sigmord函数，在学习算法中还有别的激活函数，但是sigmord函数是最常用的。这样我们就可以用sigmord神经元的输出值来表示更多东西，比如一幅图片像素强度的平均值。同样对于上文提到的用感知器决定的是否去演唱会的问题，完全可以通过定义当输出值大于0.5就去，否则就不去来建模。



